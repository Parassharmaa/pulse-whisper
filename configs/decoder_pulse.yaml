model:
  whisper_size: tiny
  freeze_whisper: true
  n_frequencies: 16
  alpha_init: 0.01
  alpha_max: 0.1
  # Decoder pulse targets all layers and heads for Whisper-Tiny (4 layers Ã— 6 heads)
  # Per-head alpha will learn which heads benefit most
  decoder_pulse: true
  use_phase_net: false

training:
  dataset: librispeech-10h
  gap_augmentation: true
  gap_fractions: [0.0, 0.05, 0.15]
  batch_size: 16
  lr: 5.0e-4
  max_epochs: 15
  fp16: true
  seed: 42
  warmup_steps: 200

logging:
  log_every_n_steps: 10

eval:
  gap_levels: [0, 5, 15, 30, multi]
  test_set: test-clean
  hallucination_test: true
